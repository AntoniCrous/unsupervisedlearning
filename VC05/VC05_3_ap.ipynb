{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>VC05: Agrupamiento basado en densidad - Affinity Propagation</h1></center>\n",
    "\n",
    "En esta práctica estudiaremos las ideas básicas que hemos visto como introducción al agrupamiento basado en densidad, propagación de afinidad.\n",
    "\n",
    "Para empezar, cargamos las librerías que vamos a necesitar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Empezamos creando el conjunto de datos con el que trabajaremos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
    "\n",
    "Dx, Dy = make_blobs(100, 2, centers=6, cluster_std=1.3)\n",
    "#Dx, Dy = make_blobs(300, 2, centers=[[1, 1], [-1, -1], [1, -1]], cluster_std = 0.5, random_state=0)\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.scatter(Dx[:,0], Dx[:,1], c = Dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como hemos estudiado en la parte teórica, el algoritmo de propagación de afinidad consta de dos pasos: la actualización de la matriz R y la de la matriz A. En el caso de la matriz R, se le resta a la matriz de similitud el valor máximo de la suma de la disponibilidad A y la similitud S entre $x_i$ cualquier otro $x_{k'}$ (para $x_i$ y $x_k$). Aprovecharemos que ese valor siempre es constante para todo $k' \\neq k$ y lo calcularemos sólo dos: para todo valor de $k'\\neq k$ y para $k$.\n",
    "\n",
    "Además, en la práctica es habitual usar un factor de aprendizaje. No se sustituye directamente el antiguo valor de R con el nuevo, sino que se hace una media ponderada (según el factor) entre los valores antiguo y nuevo de tal manera que se incorpora un $100\\times factor\\%$ del nuevo valor de R calculado y se mantiene un $100\\times (1-factor)\\%$ del antiguo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_matriz_R(mR, mA, mSimilitud, factor = 0.5):\n",
    "    rows = np.arange(mR.shape[0])\n",
    "\n",
    "    mAux = mSimilitud + mA\n",
    "    np.fill_diagonal(mAux, -np.inf)\n",
    "\n",
    "    # Obtenemos los valores máximos de cada fila\n",
    "    lIndMaximos = np.argmax(mAux, axis=1)\n",
    "    maxValor = mAux[rows, lIndMaximos]\n",
    "    mMax = np.zeros_like(mR) + maxValor[:, None]\n",
    "\n",
    "    # Obtenemos el segundo valor más alto, para k\n",
    "    mAux[rows, lIndMaximos] = -np.inf\n",
    "    maxValorK = mAux[rows, np.argmax(mAux, axis=1)]\n",
    "    mMax[rows, lIndMaximos] = maxValorK\n",
    "\n",
    "    # Conservamos el previo mR en un (1-factor)% y \n",
    "    # confiamos en la nueva estimación en un factor%\n",
    "    return (1-factor) * mR + factor * (mSimilitud - mMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En el caso de la actualización de la matriz A, hay dos casos diferentes: si $i=k$ y si $i\\neq k$. \n",
    "Básicamente, se suman las responsabilidades positivas de una serie de valores de la matriz.\n",
    "\n",
    "De igual manera, se suele usar un factor de aprendizaje que pondera entre los valores antiguo y nuevo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_matriz_A(mA, mR, factor = 0.5):\n",
    "    mA_act = np.array(mR)\n",
    "    mA_act[mA_act < 0] = 0\n",
    "    np.fill_diagonal(mA_act, np.diag(mR))\n",
    "    # se reduce a un vector\n",
    "    mA_act = mA_act.sum(axis=0)\n",
    "\n",
    "    # se repite el vector en varias columnas\n",
    "    mA_act = np.ones(mA.shape) * mA_act\n",
    "    # se elimina el que no suma (i != i')\n",
    "    mA_act -= np.clip(mR, 0, np.inf)\n",
    "    mA_act[mA_act > 0] = 0\n",
    "\n",
    "    # Calcular valores de la diagonal principal (i=k)\n",
    "    mD = np.array(mR)\n",
    "    mD[mD < 0] = 0\n",
    "    np.fill_diagonal(mD, 0)\n",
    "    \n",
    "    np.fill_diagonal(mA_act, np.sum(mD, axis=0))\n",
    "    \n",
    "    # Conservamos el previo mA en un (1-factor)% y \n",
    "    # confiamos en la nueva estimación en un factor%\n",
    "    return (1-factor) * mA + factor * mA_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una vez tenemos las dos funciones de actualización, sólo falta crear el algoritmo que de manera iterativa llama a las dos funciones previas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupamiento_propagacion_afinidad(mSimilitud, factor = 0.5, n_max_iteraciones = 400):\n",
    "    lEtiquetas = []\n",
    "\n",
    "    mR = np.array(mSimilitud)\n",
    "    mA = np.array(mSimilitud)\n",
    "\n",
    "    mFinal_ant = np.zeros(mA.shape)\n",
    "    \n",
    "    for i in np.arange(n_max_iteraciones):\n",
    "        mR = actualizar_matriz_R(mR, mA, mSimilitud, factor)\n",
    "        mA = actualizar_matriz_A(mA, mR, factor)\n",
    "        mFinal = mA + mR\n",
    "\n",
    "        lEtiquetas.append( np.argmax(mFinal, axis=1) )\n",
    "\n",
    "        if np.allclose(mFinal_ant, mFinal):\n",
    "            print(\" >>\", i, \"iteraciones hasta la convergencia <<\")\n",
    "            break\n",
    "            \n",
    "        mFinal_ant = mFinal\n",
    "        if i == (n_max_iteraciones-1):\n",
    "            print(\" >> El algoritmo para sin alcanzar la convergencia en\", n_max_iteraciones,\"iteraciones<<\")\n",
    "\n",
    "    return lEtiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Con estas tres funciones ya podemos ejecutar el algoritmo de propagación de afinidad. Sólo necesitamos calcular la matriz de similitud y asignar los valores de preferencia a la diagonal principal de dicha matriz. Un valor habitual para la preferencia suele ser la mediana de la matriz de similitud:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "mSimilitud = euclidean_distances(Dx)\n",
    "mSimilitud = -mSimilitud**2\n",
    "\n",
    "preferencia = np.median(mSimilitud) * 1 # importancia de las preferencias \n",
    "np.fill_diagonal(mSimilitud, preferencia)\n",
    "\n",
    "factor = 0.5 # factor de aprendizaje\n",
    "\n",
    "clustering = agrupamiento_propagacion_afinidad(mSimilitud, factor)\n",
    "print('Número de clústeres:', len(np.unique(Dy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "¿Cómo afecta el factor de aprendizaje a la convergencia? ¿Y cómo afectan los valores de preferencias altos? (**LAS RESPUESTAS A ESTAS PREGUNTAS LAS TENDRÁS QUE DAR EN EL CAMPUS VIRTUAL**) \n",
    "\n",
    "Añadimos una función que nos permite dibujar el proceso de búsqueda de los clústeres del agrupamiento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering(Dx, Dy):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    centros = np.unique(Dy)\n",
    "    colores = dict(zip(centros, cycle('bgrcmyk')))\n",
    "\n",
    "    for i in range(Dy.size):\n",
    "        plt.plot(Dx[i,0], Dx[i,1], 'o', markersize = 3, c = colores[Dy[i]])\n",
    "        if i in centros:\n",
    "            plt.plot(Dx[i,0], Dx[i,1], 'o', markersize = 10, markeredgecolor = 'k', c=colores[i])\n",
    "        else:\n",
    "            plt.plot([Dx[i,0], Dx[Dy[i],0]], [Dx[i,1], Dx[Dy[i],1]], c=colores[Dy[i]])\n",
    "    plt.title('Núm. clústeres: %s' % centros.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Y lo mostramos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centros_ant = np.array([])\n",
    "\n",
    "for i in np.arange(len(clustering)):\n",
    "    centros = np.unique(clustering[i])\n",
    "\n",
    "    if centros_ant.size != centros.size or np.all(centros_ant != centros):\n",
    "        plot_clustering(Dx, clustering[i])\n",
    "\n",
    "    centros_ant = centros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "También se puede estudiar la bondad de la agrupación conociendo la realidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_confusion(cat_real, cat_pred):\n",
    "    cats = np.unique(cat_real)\n",
    "    clusts = np.unique(cat_pred)\n",
    "    mat = np.array([[np.sum(np.logical_and(cat_real==cats[i], cat_pred==clusts[j])) \n",
    "                     for j in np.arange(clusts.size)] \n",
    "                    for i in np.arange(cats.size)])\n",
    "    return(mat)\n",
    "\n",
    "def medida_error(mat):\n",
    "    assign = np.sum([np.max(mat[l,:]) for l in np.arange(mat.shape[0])])\n",
    "    return 1 - assign / float(np.sum(mat))\n",
    "\n",
    "def medida_pureza(mat):\n",
    "    totales = np.sum(mat,0)/float(np.sum(mat))\n",
    "    return np.sum([totales[k] * np.max(mat[:,k]/float(np.sum(mat[:,k]))) for k in np.arange(mat.shape[1])])\n",
    "\n",
    "def medida_precision(mat, l, k):\n",
    "    return mat[l,k]/float(np.sum(mat[:,k]))\n",
    "\n",
    "def medida_recall(mat, l, k):\n",
    "    return mat[l,k]/float(np.sum(mat[l,:]))\n",
    "\n",
    "def medida_f1_especifica(mat, l, k):\n",
    "    prec = medida_precision(mat, l, k)\n",
    "    rec = medida_recall(mat, l, k)\n",
    "    if (prec+rec)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2*prec*rec/(prec+rec)\n",
    "\n",
    "def medida_f1(mat):\n",
    "    totales = np.sum(mat,1)/float(np.sum(mat))\n",
    "    assign = np.sum([totales[l] * np.max([medida_f1_especifica(mat, l, k) \n",
    "                                          for k in np.arange(mat.shape[1])]) \n",
    "                     for l in np.arange(mat.shape[0])])\n",
    "    return assign\n",
    "\n",
    "Dyp = clustering[len(clustering)-1]\n",
    "mC = matriz_confusion(Dy,Dyp)\n",
    "\n",
    "print(mC)\n",
    "print('El valor del error cometido es = ', medida_error(mC))\n",
    "print('La pureza del agrupamiento obtenido es = ', medida_pureza(mC))\n",
    "print('El valor F1 es = ', medida_f1(mC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2>Implementaciones en librerías de Python</h2>\n",
    "\n",
    "La librería ScikitLearn ya implementa el algoritmo de clustering AffinityPropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "clustering = AffinityPropagation(preference=preferencia, damping=factor).fit(Dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creamos primero una función para dibujar el resultado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_clusteringAP(modelo):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ncentros = modelo.cluster_centers_indices_.size\n",
    "    colores = 'bgrcmyk'\n",
    "\n",
    "    for k in np.arange(ncentros):\n",
    "        kc = k % len(colores)\n",
    "\n",
    "        centro = Dx[modelo.cluster_centers_indices_[k],:]\n",
    "        miembros_cluster = np.where(modelo.labels_ == k)[0]\n",
    "\n",
    "        plt.scatter(Dx[miembros_cluster, 0], Dx[miembros_cluster, 1], c=colores[kc], s=3)\n",
    "        for i in miembros_cluster:\n",
    "            plt.plot([centro[0], Dx[i,0]], [centro[1], Dx[i,1]], c = colores[kc])\n",
    "\n",
    "    plt.scatter(Dx[modelo.cluster_centers_indices_,0], Dx[modelo.cluster_centers_indices_,1], \n",
    "                s=50, c = 'black')\n",
    "\n",
    "    plt.title('Núm. clústeres: %s' % ncentros)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Podemos hacer pruebas con diferentes datasets y valores de preferencia:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_file_url = 'https://raw.githubusercontent.com/jhernandezgonzalez/unsupervisedlearning/master/datasets/sinteticos/dataset_inseparable.csv'\n",
    "#data_file_url = 'https://raw.githubusercontent.com/jhernandezgonzalez/unsupervisedlearning/master/datasets/sinteticos/dataset_dos_remolinos.csv'\n",
    "#data_file_url = 'https://raw.githubusercontent.com/jhernandezgonzalez/unsupervisedlearning/master/datasets/sinteticos/dataset_cuatro_diferente_medida.csv'\n",
    "D = np.array(pd.read_csv(data_file_url,header=0))\n",
    "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
    "Dx = D[:,0:2]\n",
    "Dy = D[:,2]\n",
    "\n",
    "factor = 0.5\n",
    "\n",
    "mSimilitud = euclidean_distances(Dx)\n",
    "mSimilitud = -mSimilitud**2\n",
    "\n",
    "preferencia = np.median(mSimilitud, axis=1)*10\n",
    "np.fill_diagonal(mSimilitud, preferencia)\n",
    "\n",
    "Dyp1 = AffinityPropagation(preference=preferencia).fit(Dx)\n",
    "dibujar_clusteringAP(Dyp1)\n",
    "\n",
    "\n",
    "preferencia = np.median(mSimilitud, axis=1)*30\n",
    "np.fill_diagonal(mSimilitud, preferencia)\n",
    "\n",
    "Dyp1 = AffinityPropagation(preference=preferencia).fit(Dx)\n",
    "dibujar_clusteringAP(Dyp1)\n",
    "\n",
    "\n",
    "preferencia = np.median(mSimilitud, axis=1)*50\n",
    "np.fill_diagonal(mSimilitud, preferencia)\n",
    "\n",
    "Dyp1 = AffinityPropagation(preference=preferencia).fit(Dx)\n",
    "dibujar_clusteringAP(Dyp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
